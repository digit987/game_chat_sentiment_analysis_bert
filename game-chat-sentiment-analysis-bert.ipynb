{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install vaderSentiment","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:26:52.761575Z","iopub.execute_input":"2023-10-28T07:26:52.762004Z","iopub.status.idle":"2023-10-28T07:26:52.766380Z","shell.execute_reply.started":"2023-10-28T07:26:52.761975Z","shell.execute_reply":"2023-10-28T07:26:52.765254Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"!pip install ktrain","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:27:10.556836Z","iopub.execute_input":"2023-10-28T07:27:10.557561Z","iopub.status.idle":"2023-10-28T07:27:10.561492Z","shell.execute_reply.started":"2023-10-28T07:27:10.557525Z","shell.execute_reply":"2023-10-28T07:27:10.560582Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"!pip install langid","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:32:21.042755Z","iopub.execute_input":"2023-10-28T05:32:21.043635Z","iopub.status.idle":"2023-10-28T05:32:33.286648Z","shell.execute_reply.started":"2023-10-28T05:32:21.043589Z","shell.execute_reply":"2023-10-28T05:32:33.285434Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: langid in /opt/conda/lib/python3.10/site-packages (1.1.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from langid) (1.23.5)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install vaderSentiment","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:32:37.732542Z","iopub.execute_input":"2023-10-28T05:32:37.732961Z","iopub.status.idle":"2023-10-28T05:32:49.870388Z","shell.execute_reply.started":"2023-10-28T05:32:37.732927Z","shell.execute_reply":"2023-10-28T05:32:49.869301Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: vaderSentiment in /opt/conda/lib/python3.10/site-packages (3.3.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vaderSentiment) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport string\nimport re\nimport langid\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport ktrain\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-28T05:33:30.612057Z","iopub.execute_input":"2023-10-28T05:33:30.612792Z","iopub.status.idle":"2023-10-28T05:33:30.621493Z","shell.execute_reply.started":"2023-10-28T05:33:30.612758Z","shell.execute_reply":"2023-10-28T05:33:30.620431Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"\"\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""},"metadata":{}}]},{"cell_type":"code","source":"df_dota = pd.read_csv(\"/kaggle/input/gosuai-dota-2-game-chats/dota2_chat_messages.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:33:40.467078Z","iopub.execute_input":"2023-10-28T05:33:40.467444Z","iopub.status.idle":"2023-10-28T05:34:11.444185Z","shell.execute_reply.started":"2023-10-28T05:33:40.467413Z","shell.execute_reply":"2023-10-28T05:34:11.443322Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_league_of_legends = pd.read_csv(\"/kaggle/input/league-of-legends-tribunal-chatlogs/chatlogs.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:34:49.226110Z","iopub.execute_input":"2023-10-28T05:34:49.226960Z","iopub.status.idle":"2023-10-28T05:34:53.341379Z","shell.execute_reply.started":"2023-10-28T05:34:49.226921Z","shell.execute_reply":"2023-10-28T05:34:53.340209Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Getting column number of column containing chat text in both dataset","metadata":{}},{"cell_type":"code","source":"df_dota.columns.get_loc('text')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:35:00.722040Z","iopub.execute_input":"2023-10-28T05:35:00.722433Z","iopub.status.idle":"2023-10-28T05:35:00.740836Z","shell.execute_reply.started":"2023-10-28T05:35:00.722401Z","shell.execute_reply":"2023-10-28T05:35:00.739691Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"code","source":"df_league_of_legends.columns.get_loc('message')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:35:02.849294Z","iopub.execute_input":"2023-10-28T05:35:02.849826Z","iopub.status.idle":"2023-10-28T05:35:02.856516Z","shell.execute_reply.started":"2023-10-28T05:35:02.849782Z","shell.execute_reply":"2023-10-28T05:35:02.855418Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"df_train = pd.DataFrame(columns = [\"chat\", \"sentiment\"])","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:35:13.768725Z","iopub.execute_input":"2023-10-28T05:35:13.769571Z","iopub.status.idle":"2023-10-28T05:35:13.775326Z","shell.execute_reply.started":"2023-10-28T05:35:13.769533Z","shell.execute_reply":"2023-10-28T05:35:13.774303Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train[\"chat\"] = pd.concat([df_dota[df_dota.columns[3]], df_league_of_legends[df_league_of_legends.columns[1]]], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:35:15.485358Z","iopub.execute_input":"2023-10-28T05:35:15.485727Z","iopub.status.idle":"2023-10-28T05:35:17.463028Z","shell.execute_reply.started":"2023-10-28T05:35:15.485699Z","shell.execute_reply":"2023-10-28T05:35:17.462165Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.sample(n=150000)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:35:28.544784Z","iopub.execute_input":"2023-10-28T05:35:28.546086Z","iopub.status.idle":"2023-10-28T05:35:30.175467Z","shell.execute_reply.started":"2023-10-28T05:35:28.546042Z","shell.execute_reply":"2023-10-28T05:35:30.174123Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:35:34.672743Z","iopub.execute_input":"2023-10-28T05:35:34.673524Z","iopub.status.idle":"2023-10-28T05:35:34.828175Z","shell.execute_reply.started":"2023-10-28T05:35:34.673489Z","shell.execute_reply":"2023-10-28T05:35:34.827284Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"          chat sentiment\ncount   150000         0\nunique   93452         0\ntop         gg       NaN\nfreq      4230       NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chat</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>93452</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>gg</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>4230</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(df_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:35:39.247646Z","iopub.execute_input":"2023-10-28T05:35:39.247983Z","iopub.status.idle":"2023-10-28T05:35:39.254826Z","shell.execute_reply.started":"2023-10-28T05:35:39.247958Z","shell.execute_reply":"2023-10-28T05:35:39.253822Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"150000"},"metadata":{}}]},{"cell_type":"markdown","source":"Removing digits, punctuation, URL and email.","metadata":{}},{"cell_type":"code","source":"def remove_digits(text):\n    return ''.join([char for char in str(text) if not char.isdigit()])\n\ndef remove_punctuation(text):\n    return ''.join([char for char in str(text) if char not in string.punctuation])\n\ndef remove_email_address(text):\n    return re.sub(\"\\S*@\\S*\\s?\", \"\", str(text))\n\ndef remove_url(text):\n    return re.sub(\"http\\S+\", \"\", str(text))\n\ndef preprocess_text(dataframe_column):\n    dataframe_column = dataframe_column.map(lambda text : remove_digits(text))\n    dataframe_column = dataframe_column.map(lambda text : remove_punctuation(text))\n    dataframe_column = dataframe_column.map(lambda text : remove_email_address(text))\n    dataframe_column = dataframe_column.map(lambda text : remove_url(text))\n    return dataframe_column","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:35:57.266865Z","iopub.execute_input":"2023-10-28T05:35:57.267632Z","iopub.status.idle":"2023-10-28T05:35:57.275467Z","shell.execute_reply.started":"2023-10-28T05:35:57.267601Z","shell.execute_reply":"2023-10-28T05:35:57.274384Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df_train['chat'] = preprocess_text(df_train['chat'])","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:35:58.902766Z","iopub.execute_input":"2023-10-28T05:35:58.903617Z","iopub.status.idle":"2023-10-28T05:36:00.366677Z","shell.execute_reply.started":"2023-10-28T05:35:58.903585Z","shell.execute_reply":"2023-10-28T05:36:00.365741Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df_train_eng = pd.DataFrame(columns = [\"chat\", \"sentiment\"])","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:36:17.387741Z","iopub.execute_input":"2023-10-28T05:36:17.388519Z","iopub.status.idle":"2023-10-28T05:36:17.393813Z","shell.execute_reply.started":"2023-10-28T05:36:17.388484Z","shell.execute_reply":"2023-10-28T05:36:17.392843Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"eng_chat = []","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:36:19.736376Z","iopub.execute_input":"2023-10-28T05:36:19.736748Z","iopub.status.idle":"2023-10-28T05:36:19.741124Z","shell.execute_reply.started":"2023-10-28T05:36:19.736721Z","shell.execute_reply":"2023-10-28T05:36:19.740249Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"for index, row in df_train.iterrows():\n    if langid.classify(row[\"chat\"])[0] == 'en':\n        eng_chat.append(row[\"chat\"])","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:36:22.110466Z","iopub.execute_input":"2023-10-28T05:36:22.110840Z","iopub.status.idle":"2023-10-28T05:40:06.675930Z","shell.execute_reply.started":"2023-10-28T05:36:22.110813Z","shell.execute_reply":"2023-10-28T05:40:06.674420Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"len(eng_chat)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:51:42.234736Z","iopub.execute_input":"2023-10-28T05:51:42.235109Z","iopub.status.idle":"2023-10-28T05:51:42.241634Z","shell.execute_reply.started":"2023-10-28T05:51:42.235078Z","shell.execute_reply":"2023-10-28T05:51:42.240780Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"81335"},"metadata":{}}]},{"cell_type":"code","source":"df_train_eng['chat'] = eng_chat","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:51:52.197699Z","iopub.execute_input":"2023-10-28T05:51:52.198062Z","iopub.status.idle":"2023-10-28T05:51:52.215814Z","shell.execute_reply.started":"2023-10-28T05:51:52.198033Z","shell.execute_reply":"2023-10-28T05:51:52.214919Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df_train_eng.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:51:55.170916Z","iopub.execute_input":"2023-10-28T05:51:55.171322Z","iopub.status.idle":"2023-10-28T05:51:55.226615Z","shell.execute_reply.started":"2023-10-28T05:51:55.171292Z","shell.execute_reply":"2023-10-28T05:51:55.225720Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"         chat sentiment\ncount   81335         0\nunique  36659         0\ntop                 NaN\nfreq     7965       NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chat</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>81335</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>36659</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td></td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>7965</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train_eng = df_train_eng.drop_duplicates(subset=['chat'], keep='first')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:51:59.785693Z","iopub.execute_input":"2023-10-28T05:51:59.786426Z","iopub.status.idle":"2023-10-28T05:51:59.808746Z","shell.execute_reply.started":"2023-10-28T05:51:59.786393Z","shell.execute_reply":"2023-10-28T05:51:59.807761Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df_train_eng.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:52:02.904077Z","iopub.execute_input":"2023-10-28T05:52:02.904486Z","iopub.status.idle":"2023-10-28T05:52:02.943279Z","shell.execute_reply.started":"2023-10-28T05:52:02.904453Z","shell.execute_reply":"2023-10-28T05:52:02.942316Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                            chat sentiment\ncount                      36659         0\nunique                     36659         0\ntop     USELESS ALCHE I EVER SAW       NaN\nfreq                           1       NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chat</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>36659</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>36659</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>USELESS ALCHE I EVER SAW</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train_eng = df_train_eng[df_train_eng['chat'].notna()]","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:52:59.077663Z","iopub.execute_input":"2023-10-28T05:52:59.078523Z","iopub.status.idle":"2023-10-28T05:52:59.091146Z","shell.execute_reply.started":"2023-10-28T05:52:59.078488Z","shell.execute_reply":"2023-10-28T05:52:59.090240Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"df_train_eng['chat'].replace('', np.nan, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:53:08.355711Z","iopub.execute_input":"2023-10-28T05:53:08.356069Z","iopub.status.idle":"2023-10-28T05:53:08.368887Z","shell.execute_reply.started":"2023-10-28T05:53:08.356039Z","shell.execute_reply":"2023-10-28T05:53:08.367888Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"No null row now","metadata":{}},{"cell_type":"code","source":"df_train_eng.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:53:13.157055Z","iopub.execute_input":"2023-10-28T05:53:13.157753Z","iopub.status.idle":"2023-10-28T05:53:13.197250Z","shell.execute_reply.started":"2023-10-28T05:53:13.157700Z","shell.execute_reply":"2023-10-28T05:53:13.196174Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"                            chat sentiment\ncount                      36658         0\nunique                     36658         0\ntop     USELESS ALCHE I EVER SAW       NaN\nfreq                           1       NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chat</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>36658</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>36658</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>USELESS ALCHE I EVER SAW</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train_eng.reset_index(drop=True, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:53:16.556123Z","iopub.execute_input":"2023-10-28T05:53:16.556548Z","iopub.status.idle":"2023-10-28T05:53:16.561295Z","shell.execute_reply.started":"2023-10-28T05:53:16.556516Z","shell.execute_reply":"2023-10-28T05:53:16.560412Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"sentiment_analyzer = SentimentIntensityAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:53:19.321610Z","iopub.execute_input":"2023-10-28T05:53:19.322350Z","iopub.status.idle":"2023-10-28T05:53:19.340122Z","shell.execute_reply.started":"2023-10-28T05:53:19.322317Z","shell.execute_reply":"2023-10-28T05:53:19.339153Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def build_row(row):\n    compound_score = sentiment_analyzer.polarity_scores(str(row['chat']))['compound']\n    if compound_score <= -0.05:\n        senti_score = 0\n    elif compound_score > -0.05 and compound_score < 0.05:\n        senti_score = 1\n    elif compound_score >= 0.05:\n        senti_score = 2\n    return senti_score","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:53:23.298683Z","iopub.execute_input":"2023-10-28T05:53:23.299089Z","iopub.status.idle":"2023-10-28T05:53:23.305151Z","shell.execute_reply.started":"2023-10-28T05:53:23.299057Z","shell.execute_reply":"2023-10-28T05:53:23.304133Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df_train_eng['sentiment'] = df_train_eng.apply(lambda row: build_row(row), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:53:27.519518Z","iopub.execute_input":"2023-10-28T05:53:27.519908Z","iopub.status.idle":"2023-10-28T05:53:29.060100Z","shell.execute_reply.started":"2023-10-28T05:53:27.519877Z","shell.execute_reply":"2023-10-28T05:53:29.058875Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"len(df_train_eng)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:55:34.918744Z","iopub.execute_input":"2023-10-28T05:55:34.919121Z","iopub.status.idle":"2023-10-28T05:55:34.925789Z","shell.execute_reply.started":"2023-10-28T05:55:34.919093Z","shell.execute_reply":"2023-10-28T05:55:34.924799Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"36659"},"metadata":{}}]},{"cell_type":"code","source":"x_train = df_train_eng['chat'][0:24439].values.tolist()\ny_train = df_train_eng['sentiment'][0:24439].values.tolist()\nx_test = df_train_eng['chat'][24439:].values.tolist()\ny_test = df_train_eng['sentiment'][24439:].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:00:24.756981Z","iopub.execute_input":"2023-10-28T06:00:24.758125Z","iopub.status.idle":"2023-10-28T06:00:24.767595Z","shell.execute_reply.started":"2023-10-28T06:00:24.758081Z","shell.execute_reply":"2023-10-28T06:00:24.766613Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print(len(x_train), len(x_train), len(x_test), len(y_test))","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:01:07.815641Z","iopub.execute_input":"2023-10-28T06:01:07.816439Z","iopub.status.idle":"2023-10-28T06:01:07.821135Z","shell.execute_reply.started":"2023-10-28T06:01:07.816406Z","shell.execute_reply":"2023-10-28T06:01:07.820273Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"24439 24439 12220 12220\n","output_type":"stream"}]},{"cell_type":"code","source":"(x_train,  y_train), (x_test, y_test), preproc = ktrain.text.texts_from_array(x_train=x_train, y_train=y_train,                                                                    \n                                                                              x_test=x_test, y_test=y_test,                                                                              \n                                                                              class_names=['0','1', '2'],\n                                                                              maxlen=128,\n                                                                              preprocess_mode='bert')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:02:05.391475Z","iopub.execute_input":"2023-10-28T06:02:05.392397Z","iopub.status.idle":"2023-10-28T06:02:15.284025Z","shell.execute_reply.started":"2023-10-28T06:02:05.392342Z","shell.execute_reply":"2023-10-28T06:02:15.283039Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n[██████████████████████████████████████████████████]\nextracting pretrained BERT model...\ndone.\n\ncleanup downloaded zip...\ndone.\n\npreprocessing train...\nlanguage: en\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"done."},"metadata":{}},{"name":"stdout","text":"Is Multi-Label? False\npreprocessing test...\nlanguage: en\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"done."},"metadata":{}},{"name":"stdout","text":"task: text classification\n","output_type":"stream"}]},{"cell_type":"code","source":"model = ktrain.text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)\nlearner = ktrain.get_learner(model, train_data=(x_train, y_train), batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:02:38.001420Z","iopub.execute_input":"2023-10-28T06:02:38.001795Z","iopub.status.idle":"2023-10-28T06:02:51.465795Z","shell.execute_reply.started":"2023-10-28T06:02:38.001767Z","shell.execute_reply":"2023-10-28T06:02:51.464644Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Is Multi-Label? False\nmaxlen is 128\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"done.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:02:54.926449Z","iopub.execute_input":"2023-10-28T06:02:54.927171Z","iopub.status.idle":"2023-10-28T06:02:55.258558Z","shell.execute_reply.started":"2023-10-28T06:02:54.927135Z","shell.execute_reply":"2023-10-28T06:02:55.257649Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n Input-Token (InputLayer)       [(None, 128)]        0           []                               \n                                                                                                  \n Input-Segment (InputLayer)     [(None, 128)]        0           []                               \n                                                                                                  \n Embedding-Token (TokenEmbeddin  [(None, 128, 768),  23440896    ['Input-Token[0][0]']            \n g)                              (30522, 768)]                                                    \n                                                                                                  \n Embedding-Segment (Embedding)  (None, 128, 768)     1536        ['Input-Segment[0][0]']          \n                                                                                                  \n Embedding-Token-Segment (Add)  (None, 128, 768)     0           ['Embedding-Token[0][0]',        \n                                                                  'Embedding-Segment[0][0]']      \n                                                                                                  \n Embedding-Position (PositionEm  (None, 128, 768)    98304       ['Embedding-Token-Segment[0][0]']\n bedding)                                                                                         \n                                                                                                  \n Embedding-Dropout (Dropout)    (None, 128, 768)     0           ['Embedding-Position[0][0]']     \n                                                                                                  \n Embedding-Norm (LayerNormaliza  (None, 128, 768)    1536        ['Embedding-Dropout[0][0]']      \n tion)                                                                                            \n                                                                                                  \n Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Embedding-Norm[0][0]']         \n on (MultiHeadAttention)                                                                          \n                                                                                                  \n Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Embedding-Norm[0][0]',         \n on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-1-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-1-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-1-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-1-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-1-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-2-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-2-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-2-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-2-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-2-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-2-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-3-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-3-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-3-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-3-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-3-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-3-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-4-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-4-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-4-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-4-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-4-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-4-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-5-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-5-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-5-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-5-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-5-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-5-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-6-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-6-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-6-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-6-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-6-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-6-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-7-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-7-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-7-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-7-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-7-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-7-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-8-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-8-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-8-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-8-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-8-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-8-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-9-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-9-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-9-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-9-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-9-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-9-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n ion (MultiHeadAttention)                                        ]']                              \n                                                                                                  \n Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n ion-Dropout (Dropout)                                           on[0][0]']                       \n                                                                                                  \n Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n ion-Add (Add)                                                   ]',                              \n                                                                  'Encoder-10-MultiHeadSelfAttenti\n                                                                 on-Dropout[0][0]']               \n                                                                                                  \n Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n                                                                                                  \n Encoder-10-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n rward)                                                          on-Norm[0][0]']                  \n                                                                                                  \n Encoder-10-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n  (Dropout)                                                                                       \n                                                                                                  \n Encoder-10-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n d)                                                              on-Norm[0][0]',                  \n                                                                  'Encoder-10-FeedForward-Dropout[\n                                                                 0][0]']                          \n                                                                                                  \n Encoder-10-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n ayerNormalization)                                              ]']                              \n                                                                                                  \n Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n ion (MultiHeadAttention)                                        0]']                             \n                                                                                                  \n Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n ion-Dropout (Dropout)                                           on[0][0]']                       \n                                                                                                  \n Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n ion-Add (Add)                                                   0]',                             \n                                                                  'Encoder-11-MultiHeadSelfAttenti\n                                                                 on-Dropout[0][0]']               \n                                                                                                  \n Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n                                                                                                  \n Encoder-11-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n rward)                                                          on-Norm[0][0]']                  \n                                                                                                  \n Encoder-11-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n  (Dropout)                                                                                       \n                                                                                                  \n Encoder-11-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n d)                                                              on-Norm[0][0]',                  \n                                                                  'Encoder-11-FeedForward-Dropout[\n                                                                 0][0]']                          \n                                                                                                  \n Encoder-11-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n ayerNormalization)                                              ]']                              \n                                                                                                  \n Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n ion (MultiHeadAttention)                                        0]']                             \n                                                                                                  \n Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n ion-Dropout (Dropout)                                           on[0][0]']                       \n                                                                                                  \n Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n ion-Add (Add)                                                   0]',                             \n                                                                  'Encoder-12-MultiHeadSelfAttenti\n                                                                 on-Dropout[0][0]']               \n                                                                                                  \n Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n                                                                                                  \n Encoder-12-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n rward)                                                          on-Norm[0][0]']                  \n                                                                                                  \n Encoder-12-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n  (Dropout)                                                                                       \n                                                                                                  \n Encoder-12-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n d)                                                              on-Norm[0][0]',                  \n                                                                  'Encoder-12-FeedForward-Dropout[\n                                                                 0][0]']                          \n                                                                                                  \n Encoder-12-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n ayerNormalization)                                              ]']                              \n                                                                                                  \n Extract (Extract)              (None, 768)          0           ['Encoder-12-FeedForward-Norm[0][\n                                                                 0]']                             \n                                                                                                  \n NSP-Dense (Dense)              (None, 768)          590592      ['Extract[0][0]']                \n                                                                                                  \n dense (Dense)                  (None, 3)            2307        ['NSP-Dense[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 109,189,635\nTrainable params: 109,189,635\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"result = learner.fit_onecycle(3e-5, 4)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:03:18.793934Z","iopub.execute_input":"2023-10-28T06:03:18.794311Z","iopub.status.idle":"2023-10-28T06:44:45.036141Z","shell.execute_reply.started":"2023-10-28T06:03:18.794281Z","shell.execute_reply":"2023-10-28T06:44:45.035103Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"\n\nbegin training using onecycle policy with max lr of 3e-05...\nEpoch 1/4\n764/764 [==============================] - 639s 802ms/step - loss: 0.4463 - accuracy: 0.8209\nEpoch 2/4\n764/764 [==============================] - 616s 806ms/step - loss: 0.1267 - accuracy: 0.9608\nEpoch 3/4\n764/764 [==============================] - 616s 806ms/step - loss: 0.0546 - accuracy: 0.9833\nEpoch 4/4\n764/764 [==============================] - 615s 806ms/step - loss: 0.0213 - accuracy: 0.9934\n","output_type":"stream"}]},{"cell_type":"code","source":"validate = learner.validate(val_data=(x_test, y_test), class_names=['0', '1', '2'])","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:48:43.411488Z","iopub.execute_input":"2023-10-28T06:48:43.412495Z","iopub.status.idle":"2023-10-28T06:50:33.278776Z","shell.execute_reply.started":"2023-10-28T06:48:43.412457Z","shell.execute_reply":"2023-10-28T06:50:33.277813Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"382/382 [==============================] - 109s 275ms/step\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      2238\n           1       0.99      0.99      0.99      7390\n           2       0.97      0.96      0.96      2592\n\n    accuracy                           0.98     12220\n   macro avg       0.97      0.97      0.97     12220\nweighted avg       0.98      0.98      0.98     12220\n\n","output_type":"stream"}]},{"cell_type":"code","source":"predictor = ktrain.get_predictor(learner.model, preproc)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:50:50.879489Z","iopub.execute_input":"2023-10-28T06:50:50.879870Z","iopub.status.idle":"2023-10-28T06:50:50.885103Z","shell.execute_reply.started":"2023-10-28T06:50:50.879840Z","shell.execute_reply":"2023-10-28T06:50:50.883943Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"predictor.get_classes()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:51:02.136126Z","iopub.execute_input":"2023-10-28T06:51:02.136505Z","iopub.status.idle":"2023-10-28T06:51:02.143021Z","shell.execute_reply.started":"2023-10-28T06:51:02.136473Z","shell.execute_reply":"2023-10-28T06:51:02.141932Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"['0', '1', '2']"},"metadata":{}}]},{"cell_type":"code","source":"prediction = predictor.predict(df_train_eng['chat'][24439:].tolist())","metadata":{"execution":{"iopub.status.busy":"2023-10-28T06:53:00.772509Z","iopub.execute_input":"2023-10-28T06:53:00.773276Z","iopub.status.idle":"2023-10-28T06:54:47.452830Z","shell.execute_reply.started":"2023-10-28T06:53:00.773240Z","shell.execute_reply":"2023-10-28T06:54:47.451906Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"df_train_eng['sentiment'][24439:].describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:16:28.527328Z","iopub.execute_input":"2023-10-28T07:16:28.527731Z","iopub.status.idle":"2023-10-28T07:16:28.540218Z","shell.execute_reply.started":"2023-10-28T07:16:28.527699Z","shell.execute_reply":"2023-10-28T07:16:28.539224Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"count    12220.000000\nmean         1.028969\nstd          0.628050\nmin          0.000000\n25%          1.000000\n50%          1.000000\n75%          1.000000\nmax          2.000000\nName: sentiment, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"print(type(prediction[0]))","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:23:49.254435Z","iopub.execute_input":"2023-10-28T07:23:49.254842Z","iopub.status.idle":"2023-10-28T07:23:49.260150Z","shell.execute_reply.started":"2023-10-28T07:23:49.254810Z","shell.execute_reply":"2023-10-28T07:23:49.259245Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"<class 'str'>\n","output_type":"stream"}]},{"cell_type":"code","source":"prediction = [int(i) for i in prediction]","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:25:00.944472Z","iopub.execute_input":"2023-10-28T07:25:00.944824Z","iopub.status.idle":"2023-10-28T07:25:00.951254Z","shell.execute_reply.started":"2023-10-28T07:25:00.944797Z","shell.execute_reply":"2023-10-28T07:25:00.950404Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"print(mean_squared_error(df_train_eng['sentiment'][24439:], prediction))","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:25:47.803352Z","iopub.execute_input":"2023-10-28T07:25:47.804240Z","iopub.status.idle":"2023-10-28T07:25:47.812155Z","shell.execute_reply.started":"2023-10-28T07:25:47.804202Z","shell.execute_reply":"2023-10-28T07:25:47.811185Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"0.044271685761047466\n","output_type":"stream"}]},{"cell_type":"code","source":"print(mean_absolute_error(df_train_eng['sentiment'][24439:], prediction))","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:26:15.840996Z","iopub.execute_input":"2023-10-28T07:26:15.841909Z","iopub.status.idle":"2023-10-28T07:26:15.850178Z","shell.execute_reply.started":"2023-10-28T07:26:15.841871Z","shell.execute_reply":"2023-10-28T07:26:15.849186Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"0.029869067103109655\n","output_type":"stream"}]}]}