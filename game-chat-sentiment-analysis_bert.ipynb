{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install vaderSentiment","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:51:50.213063Z","iopub.execute_input":"2023-10-10T08:51:50.213444Z","iopub.status.idle":"2023-10-10T08:52:02.530763Z","shell.execute_reply.started":"2023-10-10T08:51:50.213420Z","shell.execute_reply":"2023-10-10T08:52:02.529542Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vaderSentiment) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vaderSentiment) (2023.7.22)\nInstalling collected packages: vaderSentiment\nSuccessfully installed vaderSentiment-3.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install ktrain","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:52:08.967134Z","iopub.execute_input":"2023-10-10T08:52:08.967508Z","iopub.status.idle":"2023-10-10T08:52:48.256675Z","shell.execute_reply.started":"2023-10-10T08:52:08.967479Z","shell.execute_reply":"2023-10-10T08:52:48.255527Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting ktrain\n  Downloading ktrain-0.38.0.tar.gz (25.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from ktrain) (1.2.2)\nRequirement already satisfied: matplotlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from ktrain) (3.7.2)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from ktrain) (2.0.2)\nRequirement already satisfied: fastprogress>=0.1.21 in /opt/conda/lib/python3.10/site-packages (from ktrain) (1.0.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ktrain) (2.31.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from ktrain) (1.3.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ktrain) (21.3)\nCollecting langdetect (from ktrain)\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from ktrain) (0.42.1)\nCollecting cchardet (from ktrain)\n  Downloading cchardet-2.1.7.tar.gz (653 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.6/653.6 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting chardet (from ktrain)\n  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting syntok>1.3.3 (from ktrain)\n  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\nCollecting tika (from ktrain)\n  Downloading tika-2.6.0.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers>=4.17.0 in /opt/conda/lib/python3.10/site-packages (from ktrain) (4.33.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from ktrain) (0.1.99)\nCollecting keras_bert>=0.86.0 (from ktrain)\n  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting whoosh (from ktrain)\n  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras_bert>=0.86.0->ktrain) (1.23.5)\nCollecting keras-transformer==0.40.0 (from keras_bert>=0.86.0->ktrain)\n  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-pos-embd==0.13.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-multi-head==0.29.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-layer-normalization==0.16.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-position-wise-feed-forward==0.8.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-embed-sim==0.10.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting keras-self-attention==0.51.0 (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.0->ktrain) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.0->ktrain) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.0->ktrain) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.0->ktrain) (9.5.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->ktrain) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->ktrain) (2023.3)\nRequirement already satisfied: regex>2016 in /opt/conda/lib/python3.10/site-packages (from syntok>1.3.3->ktrain) (2023.6.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.17.0->ktrain) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.17.0->ktrain) (0.16.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.17.0->ktrain) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.17.0->ktrain) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.17.0->ktrain) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.17.0->ktrain) (4.66.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect->ktrain) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ktrain) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ktrain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ktrain) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ktrain) (2023.7.22)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->ktrain) (1.11.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->ktrain) (3.1.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tika->ktrain) (68.0.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.17.0->ktrain) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.17.0->ktrain) (4.6.3)\nBuilding wheels for collected packages: ktrain, keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, cchardet, langdetect, tika\n  Building wheel for ktrain (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ktrain: filename=ktrain-0.38.0-py3-none-any.whl size=25319964 sha256=6a9d4dcca0ad658a27ae077f1f69161c3925175752d8cf2bbd41f3c681b5fd32\n  Stored in directory: /root/.cache/pip/wheels/10/76/6b/5799f396ca78a8c38c7c6439a192ca88538a97cfb970946da5\n  Building wheel for keras_bert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33501 sha256=e8d4209449eee0c1aeaa0d589b82367aac03fff639176a2f2457831f188b5bb1\n  Stored in directory: /root/.cache/pip/wheels/89/0c/04/646b6fdf6375911b42c8d540a8a3fda8d5d77634e5dcbe7b26\n  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12287 sha256=b9ed473bffd8b70c1bc3297124d908e228b1b01179665cc5d1cf258d1b533dce\n  Stored in directory: /root/.cache/pip/wheels/f2/cb/22/75a0ad376129177f7c95c0d91331a18f5368fd657f4035ba7c\n  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3943 sha256=b3834e2c121d6ff827a4afb11ac1a68ae9b0a65753673b8ad85d640959389a7e\n  Stored in directory: /root/.cache/pip/wheels/82/32/c7/fd35d0d1b840a6c7cbd4343f808d10d0f7b87d271a4dbe796f\n  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4653 sha256=c685d5af95702cbbbdc75d1721c5fb9bb3225010f8a5a83a5b64834d09f57548\n  Stored in directory: /root/.cache/pip/wheels/ed/3a/4b/21db23c0cc56c4b219616e181f258eb7c57d36cc5d056fae9a\n  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14979 sha256=95ba80fb004d93e2433b279a5d0c4d45331bb36a5ea34485c84ed21c2aea593a\n  Stored in directory: /root/.cache/pip/wheels/cb/23/4b/06d7ae21714f70fcc25b48f972cc8e5e7f4b6b764a038b509d\n  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6946 sha256=c308ef676df38d20c0a5ab75c19fcc8bdab1f655431da5e7947da3072f948e67\n  Stored in directory: /root/.cache/pip/wheels/78/07/1b/b1ca47b6ac338554b75c8f52c54e6a2bfbe1b07d79579979a4\n  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4968 sha256=566bb2b28de71af0f674172796b07615ac10feedb5028c6e00d3c41f900989e7\n  Stored in directory: /root/.cache/pip/wheels/c1/6a/04/d1706a53b23b2cb5f9a0a76269bf87925daa1bca09eac01b21\n  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=1efe8cdb7e340715bfb4df34552c9811bd553cd63cd11474b5a5a21cf74160f5\n  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n  Building wheel for cchardet (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for cchardet: filename=cchardet-2.1.7-cp310-cp310-linux_x86_64.whl size=133763 sha256=03d71c1520bbb41539dc7084f6140aef677a9f4d27a863ccd7c58a60ab299498\n  Stored in directory: /root/.cache/pip/wheels/ee/e0/ab/e01326f15c59438d080b1496dbab8091e952ec72f35e3c437e\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=680f69a1b4158375b12a1df5a5e418ce73513eb336b6d42eebba2c2d015baa22\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n  Building wheel for tika (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32625 sha256=3b11f2aa7505c3f17f0172c03acbbc9d2f221cafe2772f069e8a9cdffa282fad\n  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\nSuccessfully built ktrain keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention cchardet langdetect tika\nInstalling collected packages: whoosh, cchardet, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, chardet, tika, keras-multi-head, keras-transformer, keras_bert, ktrain\nSuccessfully installed cchardet-2.1.7 chardet-5.2.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0 ktrain-0.38.0 langdetect-1.0.9 syntok-1.4.4 tika-2.6.0 whoosh-2.7.4\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport string\nimport re\nfrom langdetect import detect\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport ktrain\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n'''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-10T08:52:56.895848Z","iopub.execute_input":"2023-10-10T08:52:56.896220Z","iopub.status.idle":"2023-10-10T08:53:06.042790Z","shell.execute_reply.started":"2023-10-10T08:52:56.896189Z","shell.execute_reply":"2023-10-10T08:53:06.041321Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"\"\\nimport os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""},"metadata":{}}]},{"cell_type":"code","source":"df_dota = pd.read_csv(\"/kaggle/input/gosuai-dota-2-game-chats/dota2_chat_messages.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:53:12.861408Z","iopub.execute_input":"2023-10-10T08:53:12.862350Z","iopub.status.idle":"2023-10-10T08:53:40.136850Z","shell.execute_reply.started":"2023-10-10T08:53:12.862316Z","shell.execute_reply":"2023-10-10T08:53:40.135727Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_league_of_legends = pd.read_csv(\"/kaggle/input/league-of-legends-tribunal-chatlogs/chatlogs.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:53:42.613057Z","iopub.execute_input":"2023-10-10T08:53:42.613708Z","iopub.status.idle":"2023-10-10T08:53:46.113812Z","shell.execute_reply.started":"2023-10-10T08:53:42.613669Z","shell.execute_reply":"2023-10-10T08:53:46.112845Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Getting column number of column containing chat text in both dataset","metadata":{}},{"cell_type":"code","source":"df_dota.columns.get_loc('text')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:53:51.941624Z","iopub.execute_input":"2023-10-10T08:53:51.941961Z","iopub.status.idle":"2023-10-10T08:53:51.954706Z","shell.execute_reply.started":"2023-10-10T08:53:51.941935Z","shell.execute_reply":"2023-10-10T08:53:51.953835Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"code","source":"df_league_of_legends.columns.get_loc('message')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:53:55.454138Z","iopub.execute_input":"2023-10-10T08:53:55.454696Z","iopub.status.idle":"2023-10-10T08:53:55.461577Z","shell.execute_reply.started":"2023-10-10T08:53:55.454660Z","shell.execute_reply":"2023-10-10T08:53:55.460662Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"df_train = pd.DataFrame(columns = [\"chat\", \"sentiment\"])","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:53:59.172931Z","iopub.execute_input":"2023-10-10T08:53:59.173302Z","iopub.status.idle":"2023-10-10T08:53:59.180579Z","shell.execute_reply.started":"2023-10-10T08:53:59.173267Z","shell.execute_reply":"2023-10-10T08:53:59.179500Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_train[\"chat\"] = pd.concat([df_dota[df_dota.columns[3]], df_league_of_legends[df_league_of_legends.columns[1]]], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:02.501007Z","iopub.execute_input":"2023-10-10T08:54:02.501429Z","iopub.status.idle":"2023-10-10T08:54:04.253278Z","shell.execute_reply.started":"2023-10-10T08:54:02.501397Z","shell.execute_reply":"2023-10-10T08:54:04.252131Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.sample(n=70000)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:06.342731Z","iopub.execute_input":"2023-10-10T08:54:06.343085Z","iopub.status.idle":"2023-10-10T08:54:07.802464Z","shell.execute_reply.started":"2023-10-10T08:54:06.343057Z","shell.execute_reply":"2023-10-10T08:54:07.801468Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"len(df_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:09.766283Z","iopub.execute_input":"2023-10-10T08:54:09.767319Z","iopub.status.idle":"2023-10-10T08:54:09.774405Z","shell.execute_reply.started":"2023-10-10T08:54:09.767275Z","shell.execute_reply":"2023-10-10T08:54:09.773325Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"70000"},"metadata":{}}]},{"cell_type":"markdown","source":"Removing digits, punctuation, URL and email.","metadata":{}},{"cell_type":"code","source":"def remove_digits(text):\n    return ''.join([char for char in str(text) if not char.isdigit()])\n\ndef remove_punctuation(text):\n    return ''.join([char for char in str(text) if char not in string.punctuation])\n\ndef remove_email_address(text):\n    return re.sub(\"\\S*@\\S*\\s?\", \"\", str(text))\n\ndef remove_url(text):\n    return re.sub(\"http\\S+\", \"\", str(text))\n\ndef preprocess_text(dataframe_column):\n    dataframe_column = dataframe_column.map(lambda text : remove_digits(text))\n    dataframe_column = dataframe_column.map(lambda text : remove_punctuation(text))\n    dataframe_column = dataframe_column.map(lambda text : remove_email_address(text))\n    dataframe_column = dataframe_column.map(lambda text : remove_url(text))\n    return dataframe_column","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:14.454072Z","iopub.execute_input":"2023-10-10T08:54:14.454409Z","iopub.status.idle":"2023-10-10T08:54:14.461579Z","shell.execute_reply.started":"2023-10-10T08:54:14.454383Z","shell.execute_reply":"2023-10-10T08:54:14.460563Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_train['chat'] = preprocess_text(df_train['chat'])","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:18.589222Z","iopub.execute_input":"2023-10-10T08:54:18.589629Z","iopub.status.idle":"2023-10-10T08:54:19.021934Z","shell.execute_reply.started":"2023-10-10T08:54:18.589602Z","shell.execute_reply":"2023-10-10T08:54:19.020755Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train = df_train[df_train['chat'].notna()]","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:21.556833Z","iopub.execute_input":"2023-10-10T08:54:21.557865Z","iopub.status.idle":"2023-10-10T08:54:21.569829Z","shell.execute_reply.started":"2023-10-10T08:54:21.557811Z","shell.execute_reply":"2023-10-10T08:54:21.568802Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"No null row now","metadata":{}},{"cell_type":"code","source":"df_train['chat'].describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:26.933361Z","iopub.execute_input":"2023-10-10T08:54:26.933672Z","iopub.status.idle":"2023-10-10T08:54:26.975012Z","shell.execute_reply.started":"2023-10-10T08:54:26.933649Z","shell.execute_reply":"2023-10-10T08:54:26.974003Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"count     70000\nunique    44137\ntop            \nfreq       3703\nName: chat, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df_train = df_train.drop_duplicates(subset=['chat'], keep='first')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:29.580683Z","iopub.execute_input":"2023-10-10T08:54:29.581026Z","iopub.status.idle":"2023-10-10T08:54:29.597064Z","shell.execute_reply.started":"2023-10-10T08:54:29.580999Z","shell.execute_reply":"2023-10-10T08:54:29.595908Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_train['chat'].describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:35.430033Z","iopub.execute_input":"2023-10-10T08:54:35.430405Z","iopub.status.idle":"2023-10-10T08:54:35.459556Z","shell.execute_reply.started":"2023-10-10T08:54:35.430376Z","shell.execute_reply":"2023-10-10T08:54:35.458649Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"count                 44137\nunique                44137\ntop       мы ща чпокнем вас\nfreq                      1\nName: chat, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df_train['chat'].replace('', np.nan, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:38.589918Z","iopub.execute_input":"2023-10-10T08:54:38.590349Z","iopub.status.idle":"2023-10-10T08:54:38.605532Z","shell.execute_reply.started":"2023-10-10T08:54:38.590316Z","shell.execute_reply":"2023-10-10T08:54:38.604400Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.dropna(subset = ['chat']).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:41.213191Z","iopub.execute_input":"2023-10-10T08:54:41.213558Z","iopub.status.idle":"2023-10-10T08:54:41.228430Z","shell.execute_reply.started":"2023-10-10T08:54:41.213533Z","shell.execute_reply":"2023-10-10T08:54:41.227086Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:43.734957Z","iopub.execute_input":"2023-10-10T08:54:43.735293Z","iopub.status.idle":"2023-10-10T08:54:43.775101Z","shell.execute_reply.started":"2023-10-10T08:54:43.735236Z","shell.execute_reply":"2023-10-10T08:54:43.773916Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                     chat sentiment\ncount               44136         0\nunique              44136         0\ntop     мы ща чпокнем вас       NaN\nfreq                    1       NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chat</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>44136</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>44136</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>мы ща чпокнем вас</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train.reset_index(drop=True, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:46.660933Z","iopub.execute_input":"2023-10-10T08:54:46.661595Z","iopub.status.idle":"2023-10-10T08:54:46.666596Z","shell.execute_reply.started":"2023-10-10T08:54:46.661563Z","shell.execute_reply":"2023-10-10T08:54:46.665281Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:49.142841Z","iopub.execute_input":"2023-10-10T08:54:49.143160Z","iopub.status.idle":"2023-10-10T08:54:49.179308Z","shell.execute_reply.started":"2023-10-10T08:54:49.143135Z","shell.execute_reply":"2023-10-10T08:54:49.178150Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                     chat sentiment\ncount               44136         0\nunique              44136         0\ntop     мы ща чпокнем вас       NaN\nfreq                    1       NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chat</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>44136</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>44136</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>мы ща чпокнем вас</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sentiment_analyzer = SentimentIntensityAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:52.116822Z","iopub.execute_input":"2023-10-10T08:54:52.117869Z","iopub.status.idle":"2023-10-10T08:54:52.133425Z","shell.execute_reply.started":"2023-10-10T08:54:52.117828Z","shell.execute_reply":"2023-10-10T08:54:52.132558Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def build_row(row):\n    compound_score = sentiment_analyzer.polarity_scores(str(row['chat']))['compound']\n    if compound_score <= -0.05:\n        senti_score = 0\n    elif compound_score > -0.05 and compound_score < 0.05:\n        senti_score = 1\n    elif compound_score >= 0.05:\n        senti_score = 2\n    return senti_score","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:54.877411Z","iopub.execute_input":"2023-10-10T08:54:54.877728Z","iopub.status.idle":"2023-10-10T08:54:54.882704Z","shell.execute_reply.started":"2023-10-10T08:54:54.877702Z","shell.execute_reply":"2023-10-10T08:54:54.881736Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df_train['sentiment'] = df_train.apply(lambda row: build_row(row), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:54:57.604431Z","iopub.execute_input":"2023-10-10T08:54:57.604754Z","iopub.status.idle":"2023-10-10T08:54:58.782481Z","shell.execute_reply.started":"2023-10-10T08:54:57.604728Z","shell.execute_reply":"2023-10-10T08:54:58.781493Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"x = df_train['chat'].values.tolist()\ny = df_train['sentiment'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:55:00.949194Z","iopub.execute_input":"2023-10-10T08:55:00.949560Z","iopub.status.idle":"2023-10-10T08:55:00.955792Z","shell.execute_reply.started":"2023-10-10T08:55:00.949535Z","shell.execute_reply":"2023-10-10T08:55:00.954579Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, \n                                   random_state=42,  \n                                   test_size=0.33,  \n                                   shuffle=True) ","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:55:03.749903Z","iopub.execute_input":"2023-10-10T08:55:03.750301Z","iopub.status.idle":"2023-10-10T08:55:03.776350Z","shell.execute_reply.started":"2023-10-10T08:55:03.750236Z","shell.execute_reply":"2023-10-10T08:55:03.775250Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"len(x_train), len(y_train), len(x_test), len(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:55:06.117685Z","iopub.execute_input":"2023-10-10T08:55:06.118343Z","iopub.status.idle":"2023-10-10T08:55:06.124372Z","shell.execute_reply.started":"2023-10-10T08:55:06.118312Z","shell.execute_reply":"2023-10-10T08:55:06.123364Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(29571, 29571, 14565, 14565)"},"metadata":{}}]},{"cell_type":"code","source":"x_train = list(x_train)\ny_train = list(y_train)\nx_test = list(x_test)\ny_test = list(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:55:12.390125Z","iopub.execute_input":"2023-10-10T08:55:12.390490Z","iopub.status.idle":"2023-10-10T08:55:12.396228Z","shell.execute_reply.started":"2023-10-10T08:55:12.390464Z","shell.execute_reply":"2023-10-10T08:55:12.395303Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"(x_train,  y_train), (x_test, y_test), preproc = ktrain.text.texts_from_array(x_train=x_train, y_train=y_train,                                                                    \n                                                                              x_test=x_test, y_test=y_test,                                                                              \n                                                                              class_names=['0','1', '2'],\n                                                                              maxlen=128,\n                                                                              preprocess_mode='bert')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:55:14.695836Z","iopub.execute_input":"2023-10-10T08:55:14.696168Z","iopub.status.idle":"2023-10-10T08:55:28.349275Z","shell.execute_reply.started":"2023-10-10T08:55:14.696141Z","shell.execute_reply":"2023-10-10T08:55:28.348314Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"downloading pretrained BERT model (multi_cased_L-12_H-768_A-12.zip)...\n[██████████████████████████████████████████████████]\nextracting pretrained BERT model...\ndone.\n\ncleanup downloaded zip...\ndone.\n\npreprocessing train...\nlanguage: ru\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"done."},"metadata":{}},{"name":"stdout","text":"Is Multi-Label? False\npreprocessing test...\nlanguage: ru\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"done."},"metadata":{}},{"name":"stdout","text":"task: text classification\n","output_type":"stream"}]},{"cell_type":"code","source":"model = ktrain.text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)\nlearner = ktrain.get_learner(model, train_data=(x_train, y_train), batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:55:32.238149Z","iopub.execute_input":"2023-10-10T08:55:32.238533Z","iopub.status.idle":"2023-10-10T08:55:44.250303Z","shell.execute_reply.started":"2023-10-10T08:55:32.238507Z","shell.execute_reply":"2023-10-10T08:55:44.249365Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Is Multi-Label? False\nmaxlen is 128\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"done.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:55:46.638312Z","iopub.execute_input":"2023-10-10T08:55:46.638645Z","iopub.status.idle":"2023-10-10T08:55:46.891996Z","shell.execute_reply.started":"2023-10-10T08:55:46.638620Z","shell.execute_reply":"2023-10-10T08:55:46.891318Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n Input-Token (InputLayer)       [(None, 128)]        0           []                               \n                                                                                                  \n Input-Segment (InputLayer)     [(None, 128)]        0           []                               \n                                                                                                  \n Embedding-Token (TokenEmbeddin  [(None, 128, 768),  91812096    ['Input-Token[0][0]']            \n g)                              (119547, 768)]                                                   \n                                                                                                  \n Embedding-Segment (Embedding)  (None, 128, 768)     1536        ['Input-Segment[0][0]']          \n                                                                                                  \n Embedding-Token-Segment (Add)  (None, 128, 768)     0           ['Embedding-Token[0][0]',        \n                                                                  'Embedding-Segment[0][0]']      \n                                                                                                  \n Embedding-Position (PositionEm  (None, 128, 768)    98304       ['Embedding-Token-Segment[0][0]']\n bedding)                                                                                         \n                                                                                                  \n Embedding-Dropout (Dropout)    (None, 128, 768)     0           ['Embedding-Position[0][0]']     \n                                                                                                  \n Embedding-Norm (LayerNormaliza  (None, 128, 768)    1536        ['Embedding-Dropout[0][0]']      \n tion)                                                                                            \n                                                                                                  \n Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Embedding-Norm[0][0]']         \n on (MultiHeadAttention)                                                                          \n                                                                                                  \n Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Embedding-Norm[0][0]',         \n on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-1-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-1-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-1-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-1-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-1-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-2-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-2-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-2-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-2-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-2-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-2-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-3-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-3-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-3-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-3-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-3-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-3-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-4-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-4-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-4-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-4-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-4-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-4-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-5-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-5-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-5-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-5-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-5-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-5-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-6-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-6-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-6-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-6-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-6-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-6-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-7-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-7-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-7-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-7-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-7-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-7-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-8-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-8-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-8-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-8-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-8-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-8-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n on (MultiHeadAttention)                                         ]']                              \n                                                                                                  \n Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n on-Dropout (Dropout)                                            n[0][0]']                        \n                                                                                                  \n Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n on-Add (Add)                                                    ]',                              \n                                                                  'Encoder-9-MultiHeadSelfAttentio\n                                                                 n-Dropout[0][0]']                \n                                                                                                  \n Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n                                                                                                  \n Encoder-9-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n ward)                                                           n-Norm[0][0]']                   \n                                                                                                  \n Encoder-9-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n (Dropout)                                                                                        \n                                                                                                  \n Encoder-9-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n )                                                               n-Norm[0][0]',                   \n                                                                  'Encoder-9-FeedForward-Dropout[0\n                                                                 ][0]']                           \n                                                                                                  \n Encoder-9-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n yerNormalization)                                               ']                               \n                                                                                                  \n Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n ion (MultiHeadAttention)                                        ]']                              \n                                                                                                  \n Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n ion-Dropout (Dropout)                                           on[0][0]']                       \n                                                                                                  \n Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n ion-Add (Add)                                                   ]',                              \n                                                                  'Encoder-10-MultiHeadSelfAttenti\n                                                                 on-Dropout[0][0]']               \n                                                                                                  \n Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n                                                                                                  \n Encoder-10-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n rward)                                                          on-Norm[0][0]']                  \n                                                                                                  \n Encoder-10-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n  (Dropout)                                                                                       \n                                                                                                  \n Encoder-10-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n d)                                                              on-Norm[0][0]',                  \n                                                                  'Encoder-10-FeedForward-Dropout[\n                                                                 0][0]']                          \n                                                                                                  \n Encoder-10-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n ayerNormalization)                                              ]']                              \n                                                                                                  \n Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n ion (MultiHeadAttention)                                        0]']                             \n                                                                                                  \n Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n ion-Dropout (Dropout)                                           on[0][0]']                       \n                                                                                                  \n Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n ion-Add (Add)                                                   0]',                             \n                                                                  'Encoder-11-MultiHeadSelfAttenti\n                                                                 on-Dropout[0][0]']               \n                                                                                                  \n Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n                                                                                                  \n Encoder-11-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n rward)                                                          on-Norm[0][0]']                  \n                                                                                                  \n Encoder-11-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n  (Dropout)                                                                                       \n                                                                                                  \n Encoder-11-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n d)                                                              on-Norm[0][0]',                  \n                                                                  'Encoder-11-FeedForward-Dropout[\n                                                                 0][0]']                          \n                                                                                                  \n Encoder-11-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n ayerNormalization)                                              ]']                              \n                                                                                                  \n Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n ion (MultiHeadAttention)                                        0]']                             \n                                                                                                  \n Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n ion-Dropout (Dropout)                                           on[0][0]']                       \n                                                                                                  \n Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n ion-Add (Add)                                                   0]',                             \n                                                                  'Encoder-12-MultiHeadSelfAttenti\n                                                                 on-Dropout[0][0]']               \n                                                                                                  \n Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n                                                                                                  \n Encoder-12-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n rward)                                                          on-Norm[0][0]']                  \n                                                                                                  \n Encoder-12-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n  (Dropout)                                                                                       \n                                                                                                  \n Encoder-12-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n d)                                                              on-Norm[0][0]',                  \n                                                                  'Encoder-12-FeedForward-Dropout[\n                                                                 0][0]']                          \n                                                                                                  \n Encoder-12-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n ayerNormalization)                                              ]']                              \n                                                                                                  \n Extract (Extract)              (None, 768)          0           ['Encoder-12-FeedForward-Norm[0][\n                                                                 0]']                             \n                                                                                                  \n NSP-Dense (Dense)              (None, 768)          590592      ['Extract[0][0]']                \n                                                                                                  \n dense (Dense)                  (None, 3)            2307        ['NSP-Dense[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 177,560,835\nTrainable params: 177,560,835\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"result = learner.fit_onecycle(3e-5, 4)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T08:55:58.264034Z","iopub.execute_input":"2023-10-10T08:55:58.264655Z","iopub.status.idle":"2023-10-10T09:50:00.345282Z","shell.execute_reply.started":"2023-10-10T08:55:58.264624Z","shell.execute_reply":"2023-10-10T09:50:00.344322Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"\n\nbegin training using onecycle policy with max lr of 3e-05...\nEpoch 1/4\n925/925 [==============================] - 826s 867ms/step - loss: 0.3028 - accuracy: 0.8879\nEpoch 2/4\n925/925 [==============================] - 806s 871ms/step - loss: 0.1123 - accuracy: 0.9646\nEpoch 3/4\n925/925 [==============================] - 805s 870ms/step - loss: 0.0580 - accuracy: 0.9819\nEpoch 4/4\n925/925 [==============================] - 805s 870ms/step - loss: 0.0207 - accuracy: 0.9938\n","output_type":"stream"}]},{"cell_type":"code","source":"validate = learner.validate(val_data=(x_test, y_test), class_names=['0', '1', '2'])","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:34:31.291237Z","iopub.execute_input":"2023-10-10T10:34:31.291651Z","iopub.status.idle":"2023-10-10T10:36:57.677078Z","shell.execute_reply.started":"2023-10-10T10:34:31.291623Z","shell.execute_reply":"2023-10-10T10:36:57.676033Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"456/456 [==============================] - 138s 293ms/step\n              precision    recall  f1-score   support\n\n           0       0.94      0.96      0.95      1457\n           1       0.99      0.99      0.99     11383\n           2       0.96      0.96      0.96      1725\n\n    accuracy                           0.98     14565\n   macro avg       0.96      0.97      0.97     14565\nweighted avg       0.98      0.98      0.98     14565\n\n","output_type":"stream"}]},{"cell_type":"code","source":"predictor = ktrain.get_predictor(learner.model, preproc)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:37:19.797122Z","iopub.execute_input":"2023-10-10T10:37:19.797494Z","iopub.status.idle":"2023-10-10T10:37:19.802590Z","shell.execute_reply.started":"2023-10-10T10:37:19.797467Z","shell.execute_reply":"2023-10-10T10:37:19.801607Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"predictor.get_classes()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:37:23.008906Z","iopub.execute_input":"2023-10-10T10:37:23.009283Z","iopub.status.idle":"2023-10-10T10:37:23.016300Z","shell.execute_reply.started":"2023-10-10T10:37:23.009230Z","shell.execute_reply":"2023-10-10T10:37:23.015284Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"['0', '1', '2']"},"metadata":{}}]}]}